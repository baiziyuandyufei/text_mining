{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 实例化CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入CountVectorizer类\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 构建默认参数的CountVectorizer实例\n",
    "cv = CountVectorizer()\n",
    "# 获取实例参数\n",
    "cv.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构建文档-词矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat', 'dog', 'fish']\n",
      "[[0 1 1 1]\n",
      " [0 2 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 定义文本集\n",
    "texts = [\"dog cat fish\", \n",
    "         \"dog cat cat\", \n",
    "         \"fish bird\", \n",
    "         \"bird\",\n",
    "         \"dog\"]\n",
    "# 学习词汇表并构建文档-词矩阵\n",
    "term_document = cv.fit_transform(texts)\n",
    "# 获取词汇表\n",
    "print(cv.get_feature_names())\n",
    "# 输出文档-词矩阵\n",
    "print(term_document.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 常见参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_df：浮点数，取值范围[0.0,1.0]或整数，默认值为1.0,当构建词汇表时，词语文档频率高于max_df，则被过滤。当为整数时，词语文档频次高于max_df时，则被过滤。当vocabulary不是None时，该参数不起作用\n",
    "- min_df：浮点数，取值范围[0.0,1.0]或整数，默认为1，该参数除了指下限其他都同max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = [\"cat be\", \n",
    "         \"cat cat be\", \n",
    "         \"fish fish be\", \n",
    "         \"bird bird be\",]\n",
    "cv = CountVectorizer(min_df=2, max_df=0.9)\n",
    "cv.fit_transform(text2)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram_range:元组(min_n, max_n)抽取出ngrams的元个数的下限和上限。所有的符合min_n<=n<=max_n数量的ngrams都将被抽取出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'bird',\n",
       " 'bird be',\n",
       " 'bird bird',\n",
       " 'cat',\n",
       " 'cat be',\n",
       " 'cat cat',\n",
       " 'fish',\n",
       " 'fish be',\n",
       " 'fish fish']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "cv.fit_transform(text2)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preprocessor：可调用对象或None，默认值为None，在分词(tokenizing)和生成ngrams时覆盖预处理步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我爱', '自然语言处理']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text3 = ['我爱Love自然语言处理NLP']\n",
    "def preprocess(text):\n",
    "    p2 = re.compile(u'[^\\u4e00-\\u9fa5]')  # 中文的编码范围是：\\u4e00到\\u9fa5\n",
    "    zh = \" \".join(p2.split(text)).strip()\n",
    "    zh = \",\".join(zh.split())\n",
    "    res_str = zh  # 经过相关处理后得到中文的文本\n",
    "    return res_str\n",
    "cv = CountVectorizer(preprocessor=preprocess)\n",
    "cv.fit_transform(text3)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizer：可调用对象或None，默认值为None，在预处理(preprocessing)和生成ngrams时覆盖分词步骤。只有在参数analyzer取值为’word’时，该参数才有作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/xd/9q1skch13b5c12229mw7n9qm0000gn/T/jieba.cache\n",
      "Loading model cost 0.790 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['love', 'nlp', '处理', '我', '爱', '自然语言']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "cv = CountVectorizer(tokenizer=jieba.cut)\n",
    "cv.fit_transform(text3)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ stop_words：字符串，可选值{‘english’}，列表或None，默认值为None\n",
    " - 如果为字符串，则使用内部支持的字符串指定的语种的停用词表\n",
    " - 如果为列表，列表中的词语为停用词\n",
    " - 如果为None，不使用停用词。此时可以借助参数max_df[0.7,1.0]来根据文档频率自动检测和过滤停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'nlp', '处理', '爱', '自然语言']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words=['我'], tokenizer=jieba.cut)\n",
    "cv.fit_transform(text3)\n",
    "cv.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
